# ğŸš€ MLOps Assignment 2: End-to-End Machine Learning Pipeline

## ğŸ“Œ Project Overview

This project implements a **complete end-to-end MLOps pipeline** for predicting housing prices using the **California Housing dataset**.
The system automates **model training, evaluation, and deployment** using industry-standard MLOps tools such as **Apache Airflow, FastAPI, and Docker**.

The trained machine learning model is orchestrated via Airflow and exposed through a **RESTful API** for real-time inference, ensuring **reproducibility, scalability, and modular deployment**.

---

## ğŸ—ï¸ System Architecture

The solution consists of **two primary Dockerized services**:

### ğŸ”„ Airflow Service (Training Pipeline)

Responsible for orchestrating the ML workflow:

1. **Install Dependencies** â€“ Ensures required Python packages are available.
2. **Data Loading** â€“ Fetches the California Housing dataset using Scikit-Learn.
3. **Model Training** â€“ Trains a Linear Regression model.
4. **Evaluation & Logging** â€“ Calculates Mean Squared Error (MSE) and saves the trained model.

### ğŸŒ FastAPI Service (Inference Layer)

* Loads the trained `model.pkl`
* Exposes a `/predict` endpoint for real-time predictions
* Provides interactive API testing via **Swagger UI**

---

## ğŸ“‚ Project Structure

```bash
mlops-assignment-2/
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ train_pipeline.py     # Airflow DAG (ETL + training logic)
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py               # FastAPI application
â”‚   â””â”€â”€ Dockerfile            # API container build instructions
â”‚
â”œâ”€â”€ docker-compose.yaml       # Multi-container orchestration
â”œâ”€â”€ requirements.txt          # API dependencies
â”œâ”€â”€ model.pkl                 # Trained model (generated post-pipeline)
â””â”€â”€ README.md                 # Project documentation
```

---

## âš™ï¸ Prerequisites

Ensure the following are installed on your system:

* **Docker Desktop** (running)
* **Git**
* Minimum **8GB RAM** recommended for smooth container execution

---

## ğŸš€ Setup & Installation

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/YOUR_USERNAME/mlops-assignment-2.git
cd mlops-assignment-2
```

### 2ï¸âƒ£ Build & Start Services

```bash
docker compose up -d --build
```

â³ *First-time setup may take a few minutes as Docker images are downloaded.*

---

## ğŸ’» Usage Guide

### ğŸŸ¢ Part 1: Model Training (Airflow)

1. Open Airflow UI:
   ğŸ‘‰ [http://localhost:8080](http://localhost:8080)

2. Login credentials:

   * **Username:** airflow
   * **Password:** airflow

3. Locate the DAG: **`housing_train_pipeline`**

4. Unpause the DAG and click **â–¶ Trigger DAG**

5. Wait until all tasks turn **dark green (Success)**

6. Check logs of `log_results_task` to view the **MSE score**

---

### ğŸ” Part 2: Update the Model (Critical Step)

Since the model is trained **inside the Airflow container**, it must be copied to the host machine for the API to use.

Run after the pipeline completes:

```powershell
docker cp mlops-assignment-2-airflow-worker-1:/tmp/model.pkl ./model.pkl
```

Restart the API service:

```powershell
docker compose restart fastapi-app
```

---

### ğŸ”® Part 3: Making Predictions (FastAPI)

1. Open Swagger UI:
   ğŸ‘‰ [http://localhost:8000/docs](http://localhost:8000/docs)

2. Select **POST /predict**

3. Click **Try it out â†’ Execute**

4. View the prediction response:

```json
{
  "prediction": 4.526
}
```

---

## ğŸ› ï¸ Troubleshooting

### âŒ Localhost Refused / Empty Response

* Ensure Docker Desktop is running
* Wait **30â€“60 seconds** after startup for services to initialize

### âŒ Model Copy Error

* Ensure the Airflow pipeline completed successfully
* Copy the model immediately after training finishes
* Re-run the DAG if needed

### âŒ API Internal Server Error

* Verify `model.pkl` is not empty (file size > 0 KB)
* Re-copy the model and restart the API

---

## ğŸ“ Learning Outcomes

* **Orchestration:** Designed dependency-driven pipelines using Apache Airflow
* **Containerization:** Built and managed multi-service systems with Docker & Docker Compose
* **Model Serving:** Implemented a production-ready inference API with FastAPI
* **MLOps Best Practices:** Separation of training & serving environments, reproducibility, and automation

---

## ğŸ“Œ Technologies Used

* Python
* Scikit-Learn
* Apache Airflow
* FastAPI
* Docker & Docker Compose
* Pydantic
* REST APIs

---

## ğŸ‘¨â€ğŸ’» Author

**Muhammad Hassan Tahir**
MLOps / Machine Learning Engineer
---
